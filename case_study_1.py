# -*- coding: utf-8 -*-
"""Case Study 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VSmFiwtbsdL87ZUkIKJJ7OnfR0POnxOr
"""

import pandas as pd
import numpy as np

from google.colab import drive
import seaborn as sns

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

drive.mount('/content/drive')

"""**We perform a little EDA to familiarize ourselves with the data**

*   Since the data file is too large and I am using Google Colab, I will load the data into Google drive and read it from there since it will be easier and more efficient than reading it from local drive



"""

df = pd.read_csv("/content/drive/MyDrive/PS_20174392719_1491204439457_log.csv")

df.head()

df.isnull().sum()

df.isna().sum()

df.shape

df.describe

df['amount'].describe()

sns.scatterplot(data=df, x="step", y="amount", hue="type", style="type")

# We can calculate correlation of all the features
corr =  df[['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest','isFraud']].corr()
corr

df.columns

sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.heatmap(corr, annot=True, fmt='.1g', vmin=-1, vmax=1, center= 0, cmap= 'Dark2_r', square=True)

# merge Orig and Dest info
df['DestAmount'] = df.newbalanceDest - df.oldbalanceDest
df['OrigAmount'] = df.newbalanceOrig - df.oldbalanceOrg

df = df.drop(columns=(['nameOrig','nameDest', 'isFlaggedFraud',
                       'newbalanceDest', 'oldbalanceDest',
                       'newbalanceOrig', 'oldbalanceOrg',
                       'amount']))

df.corr()

df.type.value_counts()

#lets find the amount of fraud transactions for every type
df[df.isFraud == 1].type.value_counts()

import plotly.graph_objects as go
types=['Cash-Out', 'Transfer','Payment','Cash-in', 'Debit']

fig = go.Figure(data=[
    go.Bar(name='Non-Fraud',marker_color='lightsalmon', x=types, y=[df[df.type == 'CASH_OUT'].isFraud.value_counts()[0],
                                           df[df.type == 'TRANSFER'].isFraud.value_counts()[0],
                                           df[df.type == 'PAYMENT'].isFraud.value_counts()[0],
                                           df[df.type == 'CASH_IN'].isFraud.value_counts()[0],
                                           df[df.type == 'DEBIT'].isFraud.value_counts()[0]]),
    go.Bar(name='Fraud',marker_color='indianred', x=types, y=[df[df.type == 'CASH_OUT'].isFraud.value_counts()[1],
                                       df[df.type == 'TRANSFER'].isFraud.value_counts()[1],
                                       0,
                                           0,
                                           0])
])
# Change the bar mode
fig.update_layout(barmode='group', xaxis_tickangle=-45)
fig.show()

"""


*   we only see that fraud transactions only happen in cash_out and transfer


*   Reduce the dataset size by removing all type values that do not have fraud cases, this will improve the computation efficiency





"""

print("The original size is ",df.shape)

df = df[df.type != 'PAYMENT']
df = df[df.type != 'CASH_IN']
df = df[df.type != 'DEBIT']
df.reset_index(drop=True)

print("The current reduced size is ",df.shape)

"""

*   We have reduced the size significantly

"""

# fraud and non-fraud transactions with CASH_OUT
print(df[df.type == 'CASH_OUT'].isFraud.value_counts())

#  fraud and non-fraud transactions with TRANSFER
df[df.type == 'TRANSFER'].isFraud.value_counts()

"""# Seperating Cash and transfer"""

cash = df[df.type == 'CASH_OUT']
cash = cash.drop(columns=('type'))
cash = cash.reset_index(drop=True)

trans = df[df.type == 'TRANSFER']
trans = trans.drop(columns=('type'))
trans = trans.reset_index(drop=True)

X, y = cash[['step','DestAmount',	'OrigAmount']], cash['isFraud']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=72)

data_1 = cash.DestAmount
data_2 = cash.OrigAmount
data = [data_1, data_2] 
data

fig = plt.figure(figsize =(10, 7)) 
  
# # Creating axes instance 

sns.set_theme(style="whitegrid")
ax = sns.boxplot(x=data[0])

ax = sns.boxplot(x=data[1])

"""We can see that there are outliers present in both of the sets, with limited information we cannot distinguish between fraud cases and outliers, removing the outliers will heavily imbalance the data

# KNN
"""

x1,y1 = df[['step','DestAmount',	'OrigAmount']], df['isFraud']

X_train1, X_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.3, random_state=72)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train1)

X_train1 = scaler.transform(X_train1)
X_test1 = scaler.transform(X_test1)

X_train1

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors=4)
classifier.fit(X_train1, y_train1)

y_pred1 = classifier.predict(X_test1)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test1, y_pred1))
print(classification_report(y_test1, y_pred1))

error = []

# Calculating error for K values between 1 and 40
for i in range(1, 10):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train1, y_train1)
    pred_i = knn.predict(X_test1)
    error.append(np.mean(pred_i != y_test1))

plt.figure(figsize=(12, 6))
plt.plot(range(1, 10), error, color='blue', linestyle='solid', marker='o',
         markerfacecolor='black', markersize=10)
plt.title('Error Rate K Value')
plt.xlabel('K Value')
plt.ylabel('Mean Error')

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

clf = DecisionTreeClassifier()

clf = clf.fit(X_train1,y_train1)

y_pred1 = clf.predict(X_test1)

print(metrics.confusion_matrix(y_test1, y_pred1))
print(classification_report(y_test1, y_pred1))

from sklearn import tree

fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (10,10), dpi=300)
tree.plot_tree(clf)

"""# Conclusion 
Fraud Detection with Machine Learning becomes possible due to the ability of ML algorithms to learn from historical fraud patterns and recognize them in future transactions. These algorithms are able to find sophisticated fraud traits that a human simply cannot detect. This particular problem is binary classification, however that is not the case all the time, more often than not, they are multi-class classification problems. The difference between amounts of original account to destination account showed higher correlation to Fraud cases.

# FUTURE IMPROVEMENTS
If the strict timing constraints were lifted, I'd like to try Random Forest with KFold Cross Validation, XGBOOST, Logistic Regression (however, I doubt it would perform better than KNN). Also performing parameter and hyperparameter tuning to get out the best from the models.

I would also like to do outlier detection, since in the field of fraud detection is heavily important. Also since there is an imbalance in classes, and performing outlier detection would further skew the balance, we can try oversampling of data. This can be done using techniques like Random oversampling, SMOTE, ADASYN, etc. I believe this would further increase the evaluation metrics. The code can be found at hereThis notebook has a deeper insight and visualizations that aren't included in the website
"""